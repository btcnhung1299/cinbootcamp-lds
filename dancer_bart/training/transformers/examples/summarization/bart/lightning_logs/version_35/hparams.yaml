adam_epsilon: 1.0e-08
cache_dir: ''
config_name: ''
data_dir: ../../../../encoding/
do_predict: false
do_train: true
eval_batch_size: 32
fp16: false
fp16_opt_level: O1
gradient_accumulation_steps: 1
learning_rate: 1.0e-06
max_grad_norm: 1.0
max_source_length: 1024
max_target_length: 256
model_name_or_path: bart-large
n_gpu: 1
n_tpu_cores: 0
num_train_epochs: 3
output_dir: ./hybrid_ckpt
resume_from_checkpoint: ../../../../ckpt/hybrid/hybrid_epoch0.ckpt
seed: 42
tokenizer_name: ''
train_batch_size: 2
warmup_steps: 0
weight_decay: 0.0
